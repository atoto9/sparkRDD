{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.基礎轉換操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val data = sc.textFile(\"file:///usr/local/spark/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### map / flatMap / distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(#, Apache, Spark), Array(\"\"), Array(Spark, is, a, fast, and, general, cluster, computing, system, for, Big, Data., It, provides), Array(high-level, APIs, in, Scala,, Java,, Python,, and, R,, and, an, optimized, engine, that), Array(supports, general, computation, graphs, for, data, analysis., It, also, supports, a), Array(rich, set, of, higher-level, tools, including, Spark, SQL, for, SQL, and, DataFrames,), Array(MLlib, for, machine, learning,, GraphX, for, graph, processing,), Array(and, Spark, Streaming, for, stream, processing.), Array(\"\"), Array(<http://spark.apache.org/>), Array(\"\"), Array(\"\"), Array(##, Online, Documentation), Array(\"\"), Array(You, can, find, the, latest, Spark, documentation,, including, a, programming), Array(guide,, on,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//map函數會對每一條輸入進行指定的操作，然後為每一條輸入返回一個物件\n",
    "data.map(x => x.split(\"\\\\s+\")).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(#, Apache, Spark, \"\", Spark, is, a, fast, and, general, cluster, computing, system, for, Big, Data., It, provides, high-level, APIs, in, Scala,, Java,, Python,, and, R,, and, an, optimized, engine, that, supports, general, computation, graphs, for, data, analysis., It, also, supports, a, rich, set, of, higher-level, tools, including, Spark, SQL, for, SQL, and, DataFrames,, MLlib, for, machine, learning,, GraphX, for, graph, processing,, and, Spark, Streaming, for, stream, processing., \"\", <http://spark.apache.org/>, \"\", \"\", ##, Online, Documentation, \"\", You, can, find, the, latest, Spark, documentation,, including, a, programming, guide,, on, the, [project, web, page](http://spark.apache.org/documentation.html)., This, README, file, only, contains, basic, se..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//flatMap函數則是兩個操作的集合,正是“先映射後扁平化”：\n",
    "//操作1：同map函數一樣：對每一條輸入進行指定的操作，然後為每一條輸入返回一個物件\n",
    "//操作2：最後將所有物件合併為一個物件\n",
    "data.flatMap(x => x.split(\"\\\\s+\")).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(package, For, Programs, processing., Because, The, page](http://spark.apache.org/documentation.html)., cluster., its, [run, than, APIs, have, Try, computation, through, several, This, graph, Hive, storage, [\"Specifying, To, \"yarn\", Once, prefer, SparkPi, engine, version, file, documentation,, processing,, the, are, systems., params, not, different, refer, Interactive, R,, given., if, build, when, be, Tests, Apache, thread, programs,, including, ./bin/run-example, Spark., package., 1000).count(), Versions, HDFS, Data., >>>, page)., Maven, programming, Testing, module,, Streaming, environment, run:, Developer, clean, 1000:, rich, GraphX, Please, is, guide](http://spark.apache.org/contributing.html), run, URL,, threads., same, MASTER=spark://host:7077, on, buil..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//distinct去除重複\n",
    "data.flatMap(x => x.split(\"\\\\s+\")).distinct.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//patitions.size 查看分區數量\n",
    "data.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### eoalesce / repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//讀取資料時可指定分區數量\n",
    "val data2 = sc.textFile(\"file:///usr/local/spark/README.md\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//coalesce對RDD重新分區\n",
    "//須指定小於原分區數量，否則分區數量不變，若要大於則指定shuffle參數為true\n",
    "val rdd1 = data2.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### randomSplit / glom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd = sc.makeRDD(1 to 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//randomSplit根據權重將原RDD拆分多個RDD\n",
    "var splitRDD = rdd.randomSplit(Array(1.0,2.0,3.0,4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//randomSplit的結果是個數組\n",
    "splitRDD.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRDD(0).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 7, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRDD(1).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRDD(3).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[12] at randomSplit at <console>:22\n",
      "MapPartitionsRDD[13] at randomSplit at <console>:22\n",
      "MapPartitionsRDD[14] at randomSplit at <console>:22\n",
      "MapPartitionsRDD[15] at randomSplit at <console>:22\n"
     ]
    }
   ],
   "source": [
    "splitRDD.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(3, 9), Array(1, 7, 10), Array(4, 5, 6), Array(2, 8))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRDD.map(x => x.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd = sc.makeRDD(1 to 10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//glom將RDD中每一個分區的T類型數據轉變為元素類型為T的數組[Array[T]]\n",
    "rdd.glom().collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[18] at glom at <console>:22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### union / intersection / subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd1 = sc.makeRDD(1 to 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd2 = sc.makeRDD(3 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 3, 4, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//union將兩個RDD合併為一個RDD\n",
    "rdd1.union(rdd2).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//intersection取兩個RDD的交集並去除重複\n",
    "rdd1.intersection(rdd2).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//subtract取RDD的餘集合並不去除重複\n",
    "rdd1.subtract(rdd2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### mapPartions / mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd1 = sc.makeRDD(1 to 5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//使用mapPartitionsWithIndex對rdd1重新進行分區,帶有分區參數\n",
    "var rdd2 = rdd1.mapPartitionsWithIndex{\n",
    "    (x,iter) => {\n",
    "        var result = List[String]()\n",
    "        var i = 0\n",
    "        while(iter.hasNext){\n",
    "            i += iter.next()\n",
    "        }\n",
    "        result.::(x + \"|\" + i).iterator\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0|3, 1|12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "////使用mapPartitions對rdd1重新進行分區\n",
    "var rdd3 = rdd1.mapPartitions{\n",
    "    x => {\n",
    "        var result = List[Int]()\n",
    "        var i = 0\n",
    "        while(x.hasNext){\n",
    "            i += x.next()\n",
    "        }\n",
    "        result.::(i).iterator\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### zip / zipPartitions / zipWithIndex/ zipWithUniqueID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd1 = sc.makeRDD(1 to 5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd2 = sc.makeRDD(Seq(\"A\",\"B\",\"C\",\"D\",\"E\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,A), (2,B), (3,C), (4,D), (5,E))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//zip將兩個RDD組合成key/value型式的RDD，這裡默認partition和元素數量都相同否則拋出異常\n",
    "rdd1.zip(rdd2).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd3 = sc.makeRDD(Seq(\"A\",\"B\",\"C\",\"D\",\"E\"),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Can't zip RDDs with unequal numbers of partitions: List(2, 3)\n",
       "StackTrace:   at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//分區數量不同,出現錯誤\n",
    "rdd1.zip(rdd3).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd4 = sc.makeRDD(Seq(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 91, localhost, executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.hasNext(RDD.scala:859)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.foreach(RDD.scala:855)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.to(RDD.scala:855)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.toBuffer(RDD.scala:855)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.toArray(RDD.scala:855)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.hasNext(RDD.scala:859)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.foreach(RDD.scala:855)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.to(RDD.scala:855)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.toBuffer(RDD.scala:855)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.toArray(RDD.scala:855)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n",
       "  ... 40 elided\n",
       "Caused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.hasNext(RDD.scala:859)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.foreach(RDD.scala:855)\n",
       "  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.to(RDD.scala:855)\n",
       "  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.toBuffer(RDD.scala:855)\n",
       "  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anonfun$apply$27$$anon$2.toArray(RDD.scala:855)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//元素數量不同,出現錯誤\n",
    "rdd1.zip(rdd4).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd1 = sc.makeRDD(1 to 8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd2 = sc.makeRDD(Seq(\"A\",\"B\",\"C\",\"D\",\"E\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd3 = sc.makeRDD(Seq(\"a\",\"b\",\"c\",\"d\",\"e\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//zipPartitions函數將多個RDD按照partition組合成為新的RDD，該函數需要組合的RDD具有相同的分區數，但對於每個分區內的元素數量沒有要求。\n",
    "var rdd4 = rdd1.zipPartitions(rdd2,rdd3){\n",
    "    (rdd1Iter,rdd2Iter,rdd3Iter) => {\n",
    "        var result = List[String]()\n",
    "        while(rdd1Iter.hasNext && rdd2Iter.hasNext && rdd3Iter.hasNext){\n",
    "            result::=(rdd1Iter.next() + \"_\" + rdd2Iter.next() + \"_\" + rdd3Iter.next())            \n",
    "        }\n",
    "        result.iterator\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2_B_b, 1_A_a, 7_E_e, 6_D_d, 5_C_c)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd1 = sc.makeRDD(Seq(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var rdd2 = sc.makeRDD(Seq(\"A\",\"B\",\"R\",\"D\",\"F\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((A,0), (B,1), (R,2), (D,3), (F,4))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//zipWithIndex 將RDD中的元素和這個元素在RDD中的ID(位置)組合成key/value的型態\n",
    "rdd2.zipWithIndex().collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((A,0), (B,2), (C,4), (D,1), (E,3), (F,5))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "////zipWithUniqueId 將RDD中的元素和該元素的唯一ID(分區+位置)組合成key/value的型態\n",
    "rdd1.zipWithUniqueId().collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
